<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jian-Jian Jiang</title>

    <meta name="author" content="Jian-Jian Jiang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/Pikachu.jpg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">

              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Myself.jpg" target="_blank"><img style="width:85%;max-width:100%;object-fit: cover;border-radius: 75px" alt="profile photo" src="images/Myself.jpg" class="hoverZoomLink"></a>
              </td>

              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: left;">
                  Jian-Jian Jiang (蒋艰坚)
                </p>
                <p>
                  M.S. student
                </p>
                <p>
                  School of Computer Science and Engineering
                </p>
                <p>
                  Sun Yat-sen University
                </p>
                <p>
                  jiangjj35@mail2.sysu.edu.cn
                </p>
                <p style="text-align:left">
                  <a href="https://github.com/JianJian-Jiang" target="_blank">Github</a>
                </p>
              </td>

            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:100%; vertical-align:middle">
                <h2>
                  Biography
                </h2>
                <p>
                  I'm currently a first-year master student at <a href="https://www.sysu.edu.cn" target="_blank">Sun Yat-sen University</a>, advised by Prof. <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Wei-Shi Zheng</a>, where I cultivate the interest in research, and develop the scientific ability and taste of it.
                  Previously, I obtain my B.E. degree in <a href="https://www.hnu.edu.cn/index.htm" target="_blank">Hunan University</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:100%; vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  My current research interests focus on the efficient and reliable learning of grasping skills while ensuring their deployability in the real world. 
                  At the same time, I am actively studying how to facilitate the robot to obtain generalizable, precise and complex manipulation skills.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:80%; vertical-align:middle">
                <h2>Publications</h2>
                <p>
                  Below are my publications. My first author works are <span class="highlight">highlighted</span>.
                  (This page includes papers in arXiv, & means equal contribution, * refers to corresponding author.)
                </p>
              </td>
            </tr>
           </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:25%; vertical-align:middle">
                <h3>AI Robotics</h3>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DexGYS.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2405.19291" target="_blank">
                  <span class="papertitle">Grasp as You Say: Language-guided Dexterous Grasp Generation</span>
                </a>
                <br>
                Yi-Lin Wei, <strong>Jian-Jian Jiang</strong>, Chengyi Xing, Xiantuo Tan, Xiao-Ming Wu, Hao Li, Mark Cutkosky, Wei-Shi Zheng*.
                <br>
                <em>Neural Information Processing Systems (NeurIPS)</em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2405.19291" target="_blank">paper</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  We propose a novel task "Dexterous Grasp as You Say" (DexGYS), with a benchmark and a framework, enabling robots to perform dexterous grasping based on human commands expressed in natural language.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/R2SGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openreview.net/forum?id=uJBMZ6S02T" target="_blank">
                  <span class="papertitle">Real-to-Sim Grasp: Rethinking the Gap between Simulation and Real World in Grasp Detection</span>
                </a>
                <br>
                Jia-Feng Cai, Zibo Chen, Xiao-Ming Wu, <strong>Jian-Jian Jiang</strong>, Yi-Lin Wei, Wei-Shi Zheng*.
                <br>
                <em>Conference on Robot Learning (CoRL)</em>, 2024.
                <br>
                <a href="https://isee-laboratory.github.io/R2SGrasp" target="_blank">page</a>
                /
                <a href="https://openreview.net/forum?id=uJBMZ6S02T" target="_blank">paper</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  We propose a Real-to-Sim framework for 6-DoF Grasp detection, named R2SGrasp, with the key insight of bridging this gap in a real-to-sim way, and build a large-scale simulated dataset to pretrain our model to achieve great real-world performance.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/EconomicGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="" target="_blank">
                  <span class="papertitle">An Economic Framework for 6-DoF Grasp Detection</span>
                </a>
                <br>
                Xiao-Ming Wu&, Jiafeng Cai&, <strong>Jian-Jian Jiang</strong>, Dian Zheng, Yi-Lin Wei, Wei-Shi Zheng*
                <br>
                <em>European Conference on Computer Vision (ECCV)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2407.08366" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/EconomicGrasp" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  We propose a new economic grasping framework for 6-DoF grasp detection to economize the training resource cost and meanwhile maintain effective grasp performance, which consists of a novel label selection strategy and a focal module to enable it.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/MotionGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10764717" target="_blank">
                  <span class="papertitle">MotionGrasp: Long-Term Grasp Motion Tracking for Dynamic Grasping.</span>
                </a>
                <br>
                Nuo Chen&, Xiao-Ming Wu&, Guo-Hao Xu, <strong>Jian-Jian Jiang</strong>, Zibo Chen, Wei-Shi Zheng*.
                <br>
                <em>Robotics and Automation Letters (RA-L)</em>, 2024.
                <br>
                <a href="https://ieeexplore.ieee.org/document/10764717" target="_blank">paper</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  We design a new grasp tracking framework for dynamic grasping, fully considering long-term trajectory information and using Grasp Motion to model it well, which consists of a motion association and a motion alignment module to enable it.
                </p>
              </td>
            </tr>
          
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2>
                Academic Service
              </h2>
            </td>
          </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
